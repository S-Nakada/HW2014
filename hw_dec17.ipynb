{
 "metadata": {
  "name": "",
  "signature": "sha256:d8cce1b4d2c86e5eaf425e8ad742dc68ebc06f0c4c278e47bde410914b883773"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from quantecon import MarkovChain\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "MDP class file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MDP:\n",
      "    \n",
      "    def __init__(self, r, delta, Q):\n",
      "        self.r=np.array(r)\n",
      "        self.delta=delta\n",
      "        self.Q=np.array(Q)\n",
      "        self.n=self.r.shape[0] # number of states\n",
      "        self.m=self.r.shape[1] # number of actions\n",
      "        \n",
      "    def Bellman_Operator(self, w):\n",
      "        w=np.array(w)\n",
      "        objf=self.r+self.delta*self.Q.dot(w)\n",
      "        Bw=np.max(objf, axis=1)\n",
      "        policy=np.argmax(objf, axis=1)\n",
      "        return Bw,policy\n",
      "    \n",
      "    def solve_bellman_equation(self,w0, tol, max_iter):\n",
      "        # Iteration to obtain value function and optimal policy\n",
      "        w_current=np.array(w0)\n",
      "        for t in range(max_iter+1):\n",
      "            w_next, policy=self.Bellman_Operator(w_current)\n",
      "            if np.max(abs(w_current-w_next))<tol:\n",
      "                break\n",
      "            w_current=w_next\n",
      "\n",
      "        # Obtain transition matrix given optimal policy\n",
      "        P=np.zeros((self.n, self.n))\n",
      "        for i in range(self.n):\n",
      "            for j in range(self.n):\n",
      "                P[i,j]=self.Q[i, policy[i], j]\n",
      "        return  w_current, policy, P      \n",
      "        \n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Study example in section 5.1: Kurtz' optimal decision"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Parameters\n",
      "beta = 0.5\n",
      "delta = 0.9\n",
      "B = 10\n",
      "M = 5\n",
      "\n",
      "# Period utility function\n",
      "r = np.zeros((B+M+1, M+1))\n",
      "for s in range(B+M+1):\n",
      "    for a in range(M+1):\n",
      "        if a <= min(s, M):\n",
      "            r[s, a] = (s - a) ** beta\n",
      "        else: \n",
      "            r[s, a] = -np.inf\n",
      "        \n",
      "# Transition matrix \n",
      "Q = np.zeros((B+M+1, M+1, B+M+1))\n",
      "for s in range(B+M+1):\n",
      "    for a in range(M+1):\n",
      "        for t in range(B+M+1):\n",
      "            if a <= t <= a+B:    \n",
      "               Q[s, a, t] = 1 / (B + 1)  \n",
      "            else:\n",
      "               Q[s, a, t] = 0\n",
      "\n",
      "print(r)\n",
      "print(Q)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.                -inf        -inf        -inf        -inf        -inf]\n",
        " [ 1.          0.                -inf        -inf        -inf        -inf]\n",
        " [ 1.41421356  1.          0.                -inf        -inf        -inf]\n",
        " [ 1.73205081  1.41421356  1.          0.                -inf        -inf]\n",
        " [ 2.          1.73205081  1.41421356  1.          0.                -inf]\n",
        " [ 2.23606798  2.          1.73205081  1.41421356  1.          0.        ]\n",
        " [ 2.44948974  2.23606798  2.          1.73205081  1.41421356  1.        ]\n",
        " [ 2.64575131  2.44948974  2.23606798  2.          1.73205081  1.41421356]\n",
        " [ 2.82842712  2.64575131  2.44948974  2.23606798  2.          1.73205081]\n",
        " [ 3.          2.82842712  2.64575131  2.44948974  2.23606798  2.        ]\n",
        " [ 3.16227766  3.          2.82842712  2.64575131  2.44948974  2.23606798]\n",
        " [ 3.31662479  3.16227766  3.          2.82842712  2.64575131  2.44948974]\n",
        " [ 3.46410162  3.31662479  3.16227766  3.          2.82842712  2.64575131]\n",
        " [ 3.60555128  3.46410162  3.31662479  3.16227766  3.          2.82842712]\n",
        " [ 3.74165739  3.60555128  3.46410162  3.31662479  3.16227766  3.        ]\n",
        " [ 3.87298335  3.74165739  3.60555128  3.46410162  3.31662479  3.16227766]]\n",
        "[[[ 0.09090909  0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909  0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909\n",
        "    0.09090909]]\n",
        "\n",
        " [[ 0.09090909  0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909  0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909\n",
        "    0.09090909]]\n",
        "\n",
        " [[ 0.09090909  0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909  0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909\n",
        "    0.09090909]]\n",
        "\n",
        " ..., \n",
        " [[ 0.09090909  0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909  0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909\n",
        "    0.09090909]]\n",
        "\n",
        " [[ 0.09090909  0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909  0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909\n",
        "    0.09090909]]\n",
        "\n",
        " [[ 0.09090909  0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.09090909  0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.09090909 ...,  0.          0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.          0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909  0.        ]\n",
        "  [ 0.          0.          0.         ...,  0.09090909  0.09090909\n",
        "    0.09090909]]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Apply Bellman operator"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kurtzbellman = MDP(r, delta, Q)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "value_function, policy, P = kurtzbellman.solve_bellman_equation(np.zeros(B+M+1), tol=1e-10, max_iter=10000)\n",
      "\n",
      "print(value_function)\n",
      "print (policy)\n",
      "print (P)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 19.01740222  20.01740222  20.43161578  20.74945302  21.04078099\n",
        "  21.30873018  21.54479816  21.76928181  21.98270358  22.18824323\n",
        "  22.3845048   22.57807736  22.76109127  22.94376708  23.11533996\n",
        "  23.27761762]\n",
        "[0 0 0 0 1 1 1 2 2 3 3 4 5 5 5 5]\n",
        "[[ 0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.          0.\n",
        "   0.          0.          0.        ]\n",
        " [ 0.          0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.          0.          0.          0.        ]\n",
        " [ 0.          0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.          0.          0.          0.        ]\n",
        " [ 0.          0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.          0.          0.          0.        ]\n",
        " [ 0.          0.          0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.          0.          0.        ]\n",
        " [ 0.          0.          0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.          0.          0.        ]\n",
        " [ 0.          0.          0.          0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.          0.        ]\n",
        " [ 0.          0.          0.          0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.          0.        ]\n",
        " [ 0.          0.          0.          0.          0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.        ]\n",
        " [ 0.          0.          0.          0.          0.          0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909]\n",
        " [ 0.          0.          0.          0.          0.          0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909]\n",
        " [ 0.          0.          0.          0.          0.          0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909]\n",
        " [ 0.          0.          0.          0.          0.          0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909  0.09090909  0.09090909\n",
        "   0.09090909  0.09090909  0.09090909  0.09090909]]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "MDP policy iteration class file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MDP_pol_iter:\n",
      "    \n",
      "    def __init__(self, r, delta, Q):\n",
      "        self.r=np.array(r)\n",
      "        self.delta=delta\n",
      "        self.Q=np.array(Q)\n",
      "        self.n=self.r.shape[0] # number of states\n",
      "        self.m=self.r.shape[1] # number of actions\n",
      "    \n",
      "    \n",
      "    def value_of_policy(self, policy):\n",
      "        \n",
      "    # Obtain transition matrix given policy\n",
      "        P_pol=np.zeros((self.n, self.n))\n",
      "        policy=np.array(policy)\n",
      "        for i in range(self.n):\n",
      "            for j in range(self.n):\n",
      "                P_pol[i,j]=self.Q[i, policy[i], j]\n",
      "    \n",
      "    # Expected utility by following given policy\n",
      "        r_pol=np.zeros(self.n)\n",
      "        for i in range (self.n):\n",
      "            r_pol[i]=r[i,policy[i]]\n",
      "            \n",
      "        ex_r=np.dot(P_pol,r_pol)\n",
      "        discount=1\n",
      "    \n",
      "    # Sum of discounted expected payoff  given policy\n",
      "        v_pol=np.zeros(ex_r.shape)\n",
      "        for t in range(50):\n",
      "            v_pol=v_pol+discount*ex_r\n",
      "            ex_r=np.dot(P_pol, ex_r)\n",
      "            discount=discount*self.delta\n",
      "            \n",
      "        return v_pol\n",
      "    \n",
      "    def greedy(self,w):\n",
      "        w=np.array(w)\n",
      "        objf=self.r+self.delta*self.Q.dot(w)\n",
      "        policy=np.argmax(objf, axis=1)\n",
      "        return policy\n",
      "    \n",
      "    def policy_iteration(self, policy0,max_iter):\n",
      "        \n",
      "        policy_current=np.array(policy0)\n",
      "        for t in range(max_iter+1):\n",
      "            policy_next=self.greedy(self.value_of_policy(policy_current))\n",
      "            \n",
      "            if (policy_next==policy_current).all():\n",
      "                break\n",
      "            policy_current=policy_next\n",
      "            \n",
      "        return policy_current    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Apply this method to Kurtz' optimal decision(under construction)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kurtzsigma = MDP_pol_iter(r, delta, Q)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "policy_iter=P = kurtzsigma.policy_iteration(np.zeros(B+M+1),max_iter=10000)\n",
      "\n",
      "print(policy_iter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}